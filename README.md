# Robust linear models in Julia

| Documentation | CI Status | Coverage |
|:-------------------:|:------------------:|:-----------------:|
| [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url] | [![][CI-img]][CI-url] | [![][codecov-img]][codecov-url] |

[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg
[docs-latest-url]: https://getzze.github.io/RobustModels.jl/dev

[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg
[docs-stable-url]: https://getzze.github.io/RobustModels.jl/stable

[CI-img]: https://github.com/getzze/RobustModels.jl/workflows/CI/badge.svg
[CI-url]: https://github.com/getzze/RobustModels.jl/actions

[codecov-img]: https://img.shields.io/codecov/c/github/getzze/RobustModels.jl?label=Codecov&logo=codecov
[codecov-url]: https://codecov.io/gh/getzze/RobustModels.jl/branch/master


This package defines robust linear models using the interfaces from
[StatsBase.jl](https://github.com/JuliaStats/StatsBase.jl) and
[StatsModels.jl](https://github.com/JuliaStats/StatsModels.jl).
It defines an `AbstractRobustModel` type as a subtype of `RegressionModel` and
it defines the methods from the statistical model API like `fit`/`fit!`.

A _robust model_ is a regression model, meaning it finds a relationship between one or
several _covariates/independent variables_ `X` and a _response/dependent_ variable `y`.
Contrary to the ordinary least squares estimate, robust regression mitigates the influence
of _outliers_ in the data. A standard view for `RobustLinearModel` is to consider that the residuals
are distributed according to a _contaminated_ distribution, that is a _mixture model_
of the distribution of interest `F` and a contaminating distribution `Î”` with higher variance.
Therefore the residuals `r` follow the distribution `r ~ (1-Îµ) F + Îµ Î”`, with `0â‰¤Îµ<1`.

This package implements:

* M-estimators
* S-estimators
* MM-estimators
* Ï„-estimators
* MQuantile regression (e.g. Expectile regression)
* Robust Ridge regression (using any of the previous estimator)
* Quantile regression using interior point method
* Regularized Least Square regression

## Installation

```julia
julia>] add RobustModels
```

To install the last development version:

```julia
julia>] add RobustModels#main
```

## Usage

The prefered way of performing robust regression is by calling the `rlm` function:

`m = rlm(X, y, MEstimator{TukeyLoss}(); initial_scale=:mad)`

For quantile regression, use `quantreg`:

`m = quantreg(X, y; quantile=0.5)`

For Regularized Least Squares and a penalty term, use `rlm`:

`m = rlm(X, y, L1Penalty(); method=:cgd)`

For robust version of `mean`, `std`, `var` and `sem` statistics, specify the estimator as first argument.
Use the `dims` keyword for computing the statistics along specific dimensions.
The following functions are also implemented: `mean_and_std`, `mean_and_var` and `mean_and_sem`.

`xm = mean(MEstimator{HuberLoss}(), x; dims=2)`

`(xm, sm) = mean_and_std(TauEstimator{YohaiZamarLoss}(), x)`


## Examples

```julia
using RDatasets: dataset
using StatsModels
using RobustModels

data = dataset("robustbase", "Animals2")
data.logBrain = log.(data.Brain)
data.logBody = log.(data.Body)
form = @formula(logBrain ~ 1 + logBody)

## M-estimator using Tukey estimator
m1 = rlm(form, data, MEstimator{TukeyLoss}(); method=:cg, initial_scale=:mad)

## MM-estimator using Tukey estimator
m2 = rlm(form, data, MMEstimator{TukeyLoss}(); method=:cg, initial_scale=:L1)

## M-estimator using Huber estimator and correcting for covariate outliers using leverage
m3 = rlm(form, data, MEstimator{HuberLoss}(); method=:cg, initial_scale=:L1, correct_leverage=true)

## M-estimator using Huber estimator, providing an initial scale estimate and using Cholesky method of solving.
m4 = rlm(form, data, MEstimator{HuberLoss}(); method=:chol, initial_scale=15.0)

## S-estimator using Tukey estimator
m5 = rlm(form, data, SEstimator{TukeyLoss}(); Ïƒ0=:mad)

## Ï„-estimator using Tukey estimator
m6 = rlm(form, data, TauEstimator{TukeyLoss}(); initial_scale=:L1)

## Ï„-estimator using YohaiZamar estimator and resampling to find the global minimum
opts = Dict(:Npoints=>10, :Nsteps_Î²=>3, :Nsteps_Ïƒ=>3)
m7 = rlm(form, data, TauEstimator{YohaiZamarLoss}(); initial_scale=:L1, resample=true, resampling_options=opts)

## Expectile regression for Ï„=0.8
m8 = rlm(form, data, GeneralizedQuantileEstimator{L2Loss}(0.8))
#m8 = rlm(form, data, ExpectileEstimator(0.8))

## Refit with different parameters
refit!(m8; quantile=0.2)

## Robust ridge regression
m9 = rlm(form, data, MEstimator{TukeyLoss}(); initial_scale=:L1, ridgeÎ»=1.0)

## Quantile regression solved by linear programming interior point method
m10 = quantreg(form, data; quantile=0.2)
refit!(m10; quantile=0.8)

## Penalized regression
m11 = rlm(form, data, SquaredL2Penalty(); method=:auto)

;

# output

```

## Theory

### M-estimators

With ordinary least square (OLS), the objective function is, from maximum likelihood estimation:

`L = Â½ Î£áµ¢ (yáµ¢ - ğ’™áµ¢ ğœ·)Â² = Â½ Î£áµ¢ ráµ¢Â²`

where `yáµ¢` are the values of the response variable, `ğ’™áµ¢` are the covectors of individual covariates
(rows of the model matrix `X`), `ğœ·` is the vector of fitted coefficients and `ráµ¢` are the individual residuals.

A `RobustLinearModel` solves instead for the following loss function [1]: `L' = Î£áµ¢ Ï(ráµ¢)`
(more precisely `L' = Î£áµ¢ Ï(ráµ¢/Ïƒ)` where `Ïƒ` is an (robust) estimate of the standard deviation of the residual).
Several loss functions are implemented:

- `L2Loss`: `Ï(r) = Â½ rÂ²`, like ordinary OLS.
- `L1Loss`: `Ï(r) = |r|`, non-differentiable estimator also know as _Least absolute deviations_. Prefer the `QuantileRegression` solver.
- `HuberLoss`: `Ï(r) = if (r<c); Â½(r/c)Â² else |r|/c - Â½ end`, convex estimator that behaves as `L2` cost for small residuals and `L1` for large esiduals and outliers.
- `L1L2Loss`: `Ï(r) = âˆš(1 + (r/c)Â²) - 1`, smooth version of `HuberLoss`.
- `FairLoss`: `Ï(r) = |r|/c - log(1 + |r|/c)`, smooth version of `HuberLoss`.
- `LogcoshLoss`: `Ï(r) = log(cosh(r/c))`, smooth version of `HuberLoss`.
- `ArctanLoss`: `Ï(r) = r/c * atan(r/c) - Â½ log(1+(r/c)Â²)`, smooth version of `HuberLoss`.
- `CauchyLoss`: `Ï(r) = log(1+(r/c)Â²)`, non-convex estimator, that also corresponds to a Student's-t distribution (with fixed degree of freedom). It suppresses outliers more strongly but it is not sure to converge.
- `GemanLoss`: `Ï(r) = Â½ (r/c)Â²/(1 + (r/c)Â²)`, non-convex and bounded estimator, it suppresses outliers more strongly.
- `WelschLoss`: `Ï(r) = Â½ (1 - exp(-(r/c)Â²))`, non-convex and bounded estimator, it suppresses outliers more strongly.
- `TukeyLoss`: `Ï(r) = if r<c; â…™(1 - (1-(r/c)Â²)Â³) else â…™ end`, non-convex and bounded estimator, it suppresses outliers more strongly and it is the prefered estimator for most cases.
- `YohaiZamarLoss`: `Ï(r)` is quadratic for `r/c < 2/3` and is bounded to 1; non-convex estimator, it is optimized to have the lowest bias for a given efficiency.

The value of the tuning constants `c` are optimized for each estimator so the M-estimators have a high efficiency of 0.95. However, these estimators have a low breakdown point.

### S-estimators

Instead of minimizing `Î£áµ¢ Ï(ráµ¢/Ïƒ)`, S-estimation minimizes the estimate of the squared scale `ÏƒÂ²` with the constraint that: `1/n Î£áµ¢ Ï(ráµ¢/Ïƒ) = 1/2`.
S-estimators are only defined for bounded estimators, like `TukeyLoss`.
These estimators have low efficiency but a high breakdown point of 1/2, by choosing the tuning constant `c` accordingly.

### MM-estimators

It is a two-pass estimation, 1) Estimate `Ïƒ` using an S-estimator with high breakdown point and 2) estimate `ğœ·` using an M-estimator with high efficiency.
It results in an estimator with high efficiency and high breakdown point.

### Ï„-estimators

Like MM-estimators, Ï„-estimators combine a high efficiency with a high breakdown point.
Similar to S-estimators, it minimizes a scale estimate:
`Ï„Â² = ÏƒÂ² (2/n Î£áµ¢Ïâ‚‚(ráµ¢/Ïƒ))` where `Ïƒ` is an M-scale estimate, solution of `1/n Î£áµ¢ Ïâ‚(ráµ¢/Ïƒ) = 1/2`.
Finding the minimum of a Ï„-estimator is similar to the procedure for an S-estimator with a special weight function
that combines both functions `Ïâ‚` and `Ïâ‚‚`. To ensure a high breakdown point and high efficiency,
the two loss functions should be the same but with different tuning constants.

### MQuantile-estimators

Using an asymmetric variant of the `L1Estimator`, quantile regression is performed
(although the `QuantileRegression` solver should be prefered because it gives an exact solution).
Identically, with an M-estimator using an asymetric version of the loss function,
a generalization of quantiles is obtained. For instance, using an asymetric `L2Loss` results in _Expectile Regression_.

### Quantile regression

_Quantile regression_ results from minimizing the following objective function:
`L = Î£áµ¢ wáµ¢|yáµ¢ - ğ’™áµ¢ ğœ·| = Î£áµ¢ wáµ¢(ráµ¢) |ráµ¢|`,
where `wáµ¢ = ifelse(ráµ¢>0, Ï„, 1-Ï„)` and `Ï„` is the quantile of interest. `Ï„=0.5` corresponds to _Least Absolute Deviations_.

This problem is solved exactly using linear programming techniques,
specifically, interior point methods using the internal API of [Tulip](https://github.com/ds4dm/Tulip.jl).
The internal API is considered unstable, but it results in a much lighter dependency than
including the [JuMP](https://github.com/JuliaOpt/JuMP.jl) package with Tulip backend.

### Robust Ridge regression

This is the robust version of the ridge regression, adding a penalty on the coefficients.
The objective function to solve is `L = Î£áµ¢ Ï(ráµ¢/Ïƒ) + Î» Î£â±¼|Î²â±¼|Â²`, where the sum of squares of
coefficients does not include the intersect `Î²â‚€`.
Robust ridge regression is implemented for all the estimators (not for `quantreg`).
By default, all the coefficients (except the intercept) have the same penalty, which assumes that
all the feature variables have the same scale. If it is not the case, use a robust estimate of scale
to normalize every column of the model matrix `X` before fitting the regression.

### Regularized Least Squares

_Regularized Least Squares_ regression is the solution of the minimization of following objective function:
`L = Â½ Î£áµ¢ |yáµ¢ - ğ’™áµ¢ ğœ·|Â² + P(ğœ·)`,
where `P(ğœ·)` is a (sparse) penalty on the coefficients.

The following penalty function are defined:
    - `NoPenalty`: `cost(ğ±) = 0`, no penalty.
    - `SquaredL2Penalty`: `cost(ğ±) = Î» Â½||ğ±||â‚‚Â²`, also called Ridge.
    - `L1Penalty`: `cost(ğ±) = Î»||ğ±||â‚`, also called LASSO.
    - `ElasticNetPenalty`: `cost(ğ±) = Î» . l1_ratio .||ğ±||â‚ + Î» .(1 - l1_ratio) . Â½||ğ±||â‚‚Â²`.
    - `EuclideanPenalty`: `cost(ğ±) = Î»||ğ±||â‚‚`, non-separable penalty used in Group LASSO.

Different penalties can be applied to different indices of the coefficients, using
`RangedPenalties(ranges, penalties)`. E.g., `RangedPenalties([2:5], [L1Penalty(1.0)])` defines
a L1 penalty on every coefficients except the first index, which can correspond to the intercept.

With a penalty, the following solvers are available (instead of the other ones):
    - `:cgd`, Coordinate Gradient Descent [2].
    - `:fista`, Fast Iterative Shrinkage-Thresholding Algorithm [3].
    - `:ama`, Alternating Minimization Algorithm [4].
    - `:admm`, Alternating Direction Method of Multipliers [5].


## Credits

This package derives from the [RobustLeastSquares](https://github.com/FugroRoames/RobustLeastSquares.jl)
package for the initial implementation, especially for the Conjugate Gradient
solver and the definition of the M-Estimator functions.

Credits to the developpers of the [GLM](https://github.com/JuliaStats/GLM.jl)
and [MixedModels](https://github.com/JuliaStats/MixedModels.jl) packages
for implementing the Iteratively Reweighted Least Square algorithm.

## References

[1] "Robust Statistics: Theory and Methods (with R)", 2nd Edition, 2019, R. Maronna, R. Martin, V. Yohai, M. SalibiÃ¡n-Barrera
[2] "Regularization Paths for Generalized Linear Models via Coordinate Descent", 2009, J. Friedman, T. Hastie, R. Tibshirani
[3] "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems", 2009, A. Beck, M. Teboulle
[4] "Applications of a splitting algorithm to decomposition in convex programming and variational inequalities", 1991, P. Tseng
[5] "Fast Alternating Direction Optimization Methods", 2014, T. Goldstein, B. O'Donoghue, S. Setzer, R. Baraniuk
