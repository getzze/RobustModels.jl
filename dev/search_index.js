var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Types-defined-in-the-package","page":"API","title":"Types defined in the package","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"AbstractRobustModel\nAbstractEstimator\nAbstractQuantileEstimator\nLossFunction\nRobustLinearModel\nRobustModels.DensePredCG\nRobustModels.SparsePredCG\nGLM.DensePredChol\nGLM.SparsePredChol\nRobustModels.RidgePred\nRobustModels.RobustLinResp\nQuantileRegression","category":"page"},{"location":"api/#RobustModels.AbstractRobustModel","page":"API","title":"RobustModels.AbstractRobustModel","text":"AbstractRobustModel\n\nAbstract type for robust models.\n\nRobustModels.jl implements one subtype: RobustLinearModel. See the documentation for each for more details.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.AbstractEstimator","page":"API","title":"RobustModels.AbstractEstimator","text":"A robust estimator is a location or scale estimator associated to one (or more) loss function and solved using Iteratively Reweighted Least Square.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.AbstractQuantileEstimator","page":"API","title":"RobustModels.AbstractQuantileEstimator","text":"Generalized M-Quantile estimator\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.LossFunction","page":"API","title":"RobustModels.LossFunction","text":"An estimator needs a cost/loss function for the modified (weighted) least squares problems of the form:\n\nmin sum_i rholeft(dfracr_ihatsigma\right)\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.RobustLinearModel","page":"API","title":"RobustModels.RobustLinearModel","text":"RobustLinearModel\n\nRobust linear model representation\n\nFields\n\nresp: the RobustLinResp structure.\npred: the predictor structure, of type DensePredChol, SparsePredChol, DensePredCG, SparsePredCG or RidgePred.\nfitdispersion: if true, the dispersion is estimated otherwise it is kept fixed\nfitted: if true, the model was already fitted\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.DensePredCG","page":"API","title":"RobustModels.DensePredCG","text":"DensePredCG\n\nA LinPred type with Conjugate Gradient and a dense X\n\nMembers\n\nX: Model matrix of size n × p with n ≥ p.  Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.SparsePredCG","page":"API","title":"RobustModels.SparsePredCG","text":"SparsePredCG\n\nA LinPred type with Conjugate Gradient and a sparse X\n\nMembers\n\nX: Model matrix of size n × p with n ≥ p.  Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.DensePredChol","page":"API","title":"GLM.DensePredChol","text":"DensePredChol{T}\n\nA LinPred type with a dense Cholesky factorization of X'X\n\nMembers\n\nX: model matrix of size n × p with n ≥ p.  Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\nchol: a Cholesky object created from X'X, possibly using row weights.\nscratchm1: scratch Matrix{T} of the same size as X\nscratchm2: scratch Matrix{T} os the same size as X'X\n\n\n\n\n\n","category":"type"},{"location":"api/#GLM.SparsePredChol","page":"API","title":"GLM.SparsePredChol","text":"SparsePredChol{T<:BlasReal} <: LinPred\n\nA LinPred type with a sparse Cholesky factorization of X'X\n\nMembers\n\nX: model matrix of size n×p with n ≥ p. Should be full column rank.\nbeta0: base coefficient vector of length p\ndelbeta: increment to coefficient vector, also of length p\nscratchbeta: scratch vector of length p, used in linpred! method\nchol: a Cholesky object created from X'X, possibly using row weights.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.RidgePred","page":"API","title":"RobustModels.RidgePred","text":"RidgePred\n\nRegularized predictor using ridge regression on the p features.\n\nMembers\n\nX: model matrix\nλ: shrinkage parameter of the regularizer\nG: regularizer matrix of size p×p.\nβprior: regularizer prior of the coefficient values. Default to zeros(p).\npred: the non-regularized predictor using an extended model matrix.\npivot: for DensePredChol, if the decomposition was pivoted.\nscratchbeta: scratch vector of length p, used in linpred! method\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.RobustLinResp","page":"API","title":"RobustModels.RobustLinResp","text":"RobustLinResp\n\nRobust linear response structure.\n\nSolve the following minimization problem:\n\nmin sum_i rholeft(dfracr_ihatsigma\right)\n\nFields\n\nest: estimator used for the model\ny: response vector\nμ: mean response vector\noffset: offset added to Xβ to form μ. Can be of length 0\nwts: prior case weights.  Can be of length 0.\nσ: current estimate of the scale or dispersion\ndevresid: the deviance residuals\nwrkwt: working case weights for the Iteratively Reweighted Least Squares (IRLS) algorithm\nwrkres: working residuals for IRLS\nwrkscaledres: scaled residuals for IRLS\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.QuantileRegression","page":"API","title":"RobustModels.QuantileRegression","text":"QuantileRegression\n\nQuantile regression representation\n\nFields\n\nτ: the quantile value\nX: the model matrix\nβ: the coefficients\ny: the response vector\nwts: the weights\nwrkres: the working residuals\nfitdispersion: if true, the dispersion is estimated otherwise it is kept fixed\nfitted: if true, the model was already fitted\n\n\n\n\n\n","category":"type"},{"location":"api/#Constructors-for-models","page":"API","title":"Constructors for models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"fit(::Type{M}, ::Union{AbstractMatrix{T},SparseMatrixCSC{T}}, ::AbstractVector{T}, ::AbstractEstimator) where {T<:AbstractFloat, M<:RobustLinearModel}\nfit(::Type{M}, ::Union{AbstractMatrix{T},SparseMatrixCSC{T}}, ::AbstractVector{T}) where {T<:AbstractFloat, M<:QuantileRegression}","category":"page"},{"location":"api/#StatsBase.fit-Union{Tuple{M}, Tuple{T}, Tuple{Type{M}, AbstractMatrix{T}, AbstractVector{T}, AbstractEstimator}} where {T<:AbstractFloat, M<:RobustLinearModel}","page":"API","title":"StatsBase.fit","text":"fit(::Type{M},\n    X::Union{AbstractMatrix{T},SparseMatrixCSC{T}},\n    y::AbstractVector{T},\n    est::Estimator;\n    method::Symbol       = :chol, # :cg\n    dofit::Bool          = true,\n    wts::FPVector        = similar(y, 0),\n    offset::FPVector     = similar(y, 0),\n    fitdispersion::Bool  = false,\n    ridgeλ::Real         = 0,\n    ridgeG::Union{UniformScaling, AbstractArray} = I,\n    βprior::AbstractVector = [],\n    quantile::Union{Nothing, AbstractFloat} = nothing,\n    initial_scale::Union{Symbol, Real}=:mad,\n    σ0::Union{Nothing, Symbol, Real}=initial_scale,\n    initial_coef::AbstractVector=[], \n    β0::AbstractVector=initial_coef, \n    correct_leverage::Bool=false\n    fitargs...) where {M<:RobustLinearModel, T<:AbstractFloat}\n\nCreate a robust model with the model matrix (or formula) X and response vector (or dataframe) y, using a robust estimator.\n\nArguments\n\nX: the model matrix (it can be dense or sparse) or a formula\ny: the response vector or a dataframe.\nest: a robust estimator\n\nKeywords\n\nmethod::Symbol = :chol: the method to use for solving the weighted linear system, chol (default) or cg;\ndofit::Bool = true: if false, return the model object without fitting;\nwts::Vector = []: a weight vector, should be empty if no weights are used;\noffset::Vector = []: an offset vector, should be empty if no offset is used;\nfitdispersion::Bool = false: reevaluate the dispersion;\nridgeλ::Real = 0: if positive, perform a robust ridge regression with shrinkage parameter ridgeλ. RidgePred object will be used;\nridgeG::Union{UniformScaling, AbstractArray} = I: define a custom regularization matrix. Default to unity matrix (with 0 for the intercept);\nβprior::AbstractVector = []: define a custom prior for the coefficients for ridge regression. Default to zeros(p);\nquantile::Union{Nothing, AbstractFloat} = nothing: only for GeneralizedQuantileEstimator, define the quantile to estimate;\ninitial_scale::Union{Symbol, Real}=:mad: the initial scale estimate, for non-convex estimator it helps to find the global minimum. Automatic computation using :mad, L1 or extrema (non-robust).\nσ0::Union{Nothing, Symbol, Real}=initial_scale: alias of initial_scale;\ninitial_coef::AbstractVector=[]: the initial coefficients estimate, for non-convex estimator it helps to find the global minimum.\nβ0::AbstractVector=initial_coef: alias of initial_coef;\ncorrect_leverage::Bool=false: apply the leverage correction weights with leverage_weights. \nfitargs...: other keyword arguments used to control the convergence of the IRLS algorithm (see pirls!).\n\nOutput\n\nthe RobustLinearModel object.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.fit-Union{Tuple{M}, Tuple{T}, Tuple{Type{M}, AbstractMatrix{T}, AbstractVector{T}}} where {T<:AbstractFloat, M<:QuantileRegression}","page":"API","title":"StatsBase.fit","text":"fit(::Type{M}, X::Union{AbstractMatrix{T},SparseMatrixCSC{T}},\n    y::AbstractVector{T}; quantile::AbstractFloat=0.5,\n    dofit::Bool          = true,\n    wts::FPVector        = similar(y, 0),\n    fitdispersion::Bool  = false,\n    fitargs...) where {M<:QuantileRegression, T<:AbstractFloat}\n\nFit a quantile regression model with the model matrix (or formula) X and response vector (or dataframe) y.\n\nIt is solved using the exact interior method.\n\nArguments\n\nX: the model matrix (it can be dense or sparse) or a formula\ny: the response vector or a dataframe.\n\nKeywords\n\nquantile::AbstractFloat=0.5: the quantile value for the regression, between 0 and 1.\ndofit::Bool = true: if false, return the model object without fitting;\nwts::Vector = []: a weight vector, should be empty if no weights are used;\nfitdispersion::Bool = false: reevaluate the dispersion;\nfitargs...: other keyword arguments like verbose to print iteration details.\n\nOutput\n\nthe RobustLinearModel object.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"rlm\nquantreg\nfit!\nrefit!","category":"page"},{"location":"api/#RobustModels.rlm","page":"API","title":"RobustModels.rlm","text":"rlm(X, y, args...; kwargs...)\n\nAn alias for fit(RobustLinearModel, X, y, est; kwargs...).\n\nThe arguments X and y can be a Matrix and a Vector or a Formula and a DataFrame.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.quantreg","page":"API","title":"RobustModels.quantreg","text":"quantreg(X, y, args...; kwargs...)\n\nAn alias for fit(QuantileRegression, X, y; kwargs...).\n\nThe arguments X and y can be a Matrix and a Vector or a Formula and a DataFrame.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.fit!","page":"API","title":"StatsBase.fit!","text":"fit!(m::RobustLinearModel; initial_scale::Union{Symbol, Real}=:mad,\n          σ0::Union{Nothing, Symbol, Real}=initial_scale,\n          initial_coef::AbstractVector=[], \n          β0::AbstractVector=initial_coef, \n          correct_leverage::Bool=false, kwargs...)\n\nOptimize the objective of a RobustLinearModel.  When verbose is true the values of the objective and the parameters are printed on stdout at each iteration.\n\nThis function assumes that m was correctly initialized.\n\nThis function returns early if the model was already fitted, instead call refit!.\n\n\n\n\n\nfit!(m::QuantileRegression;\n     verbose::Bool=false,\n     quantile::Union{Nothing, AbstractFloat}=nothing,\n     correct_leverage::Bool=false,\n     kwargs...)\n\nOptimize the objective of a QuantileRegression.  When verbose is true the values of the objective and the parameters are printed on stdout at each function evaluation.\n\nThis function assumes that m was correctly initialized. This function returns early if the model was already fitted, instead call refit!.\n\n\n\n\n\nFit a statistical model in-place.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.refit!","page":"API","title":"RobustModels.refit!","text":"refit!(m::RobustLinearModel, [y::FPVector];\n                             wts::Union{Nothing, FPVector} = nothing,\n                             offset::Union{Nothing, FPVector} = nothing,\n                             quantile::Union{Nothing, AbstractFloat} = nothing,\n                             ridgeλ::Union{Nothing, Real} = nothing,\n                             kwargs...)\n\nRefit the RobustLinearModel.\n\nThis function assumes that m was correctly initialized and the model is refitted with the new values for the response, weights, offset, quantile and ridge shrinkage.\n\nDefining a new quantile is only possible for GeneralizedQuantileEstimator.\n\nDefining a new ridgeλ is only possible for RidgePred objects.\n\n\n\n\n\nrefit!(m::QuantileRegression, [y::FPVector ; verbose::Bool=false, quantile::Union{Nothing, AbstractFloat}=nothing])\n\nRefit the QuantileRegression model with the new values for the response, weights and quantile. This function assumes that m was correctly initialized.\n\n\n\n\n\n","category":"function"},{"location":"api/#Model-methods","page":"API","title":"Model methods","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"StatsBase.coef\nStatsBase.coeftable\nStatsBase.confint\nStatsBase.deviance\nStatsBase.nulldeviance\nStatsBase.dof\nStatsBase.dof_residual\nnobs(::StatisticalModel)\nStatsBase.isfitted\nStatsBase.islinear\nStatsBase.loglikelihood\nStatsBase.nullloglikelihood\nStatsBase.stderror\nStatsBase.vcov\nStatsBase.weights\nworkingweights\nStatsBase.fitted\nStatsBase.predict\nStatsBase.leverage\nStatsBase.modelmatrix\nprojectionmatrix\nGLM.dispersion(::RobustLinearModel, ::Bool)\nStatsBase.response\nStatsBase.residuals\nscale\ntauscale\nRobustModels.location_variance\nEstimator","category":"page"},{"location":"api/#StatsBase.coef","page":"API","title":"StatsBase.coef","text":"coef(model::StatisticalModel)\n\nReturn the coefficients of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.coeftable","page":"API","title":"StatsBase.coeftable","text":"coeftable(model::StatisticalModel; level::Real=0.95)\n\nReturn a table with coefficients and related statistics of the model. level determines the level for confidence intervals (by default, 95%).\n\nThe returned CoefTable object implements the Tables.jl interface, and can be converted e.g. to a DataFrame via using DataFrames; DataFrame(coeftable(model)).\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.confint","page":"API","title":"StatsBase.confint","text":"confint(model::StatisticalModel; level::Real=0.95)\n\nCompute confidence intervals for coefficients, with confidence level level (by default 95%).\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.deviance","page":"API","title":"StatsBase.deviance","text":"deviance(m::RobustLinearModel)\n\nThe sum of twice the loss/objective applied to the scaled residuals.\n\nIt is consistent with the definition of the deviance for OLS.\n\n\n\n\n\ndeviance(obj::LinearModel)\n\nFor linear models, the deviance is equal to the residual sum of squares (RSS).\n\n\n\n\n\ndeviance(model::StatisticalModel)\n\nReturn the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal, up to a constant, to -2 log L, with L the likelihood of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.nulldeviance","page":"API","title":"StatsBase.nulldeviance","text":"nulldeviance(obj::LinearModel)\n\nFor linear models, the deviance of the null model is equal to the total sum of squares (TSS).\n\n\n\n\n\nnulldeviance(model::StatisticalModel)\n\nReturn the deviance of the null model, that is the one including only the intercept.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.dof","page":"API","title":"StatsBase.dof","text":"dof(model::StatisticalModel)\n\nReturn the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.dof_residual","page":"API","title":"StatsBase.dof_residual","text":"dof_residual(model::RegressionModel)\n\nReturn the residual degrees of freedom of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.nobs-Tuple{StatisticalModel}","page":"API","title":"StatsBase.nobs","text":"nobs(model::StatisticalModel)\n\nReturn the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.isfitted","page":"API","title":"StatsBase.isfitted","text":"isfitted(model::StatisticalModel)\n\nIndicate whether the model has been fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.islinear","page":"API","title":"StatsBase.islinear","text":"islinear(model::StatisticalModel)\n\nIndicate whether the model is linear.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.loglikelihood","page":"API","title":"StatsBase.loglikelihood","text":"loglikelihood(model::StatisticalModel)\n\nReturn the log-likelihood of the model.\n\n\n\n\n\nloglikelihood(model::StatisticalModel, ::Colon)\n\nReturn a vector of each observation's contribution to the log-likelihood of the model. In other words, this is the vector of the pointwise log-likelihood contributions.\n\nIn general, sum(loglikehood(model, :)) == loglikelihood(model).\n\n\n\n\n\nloglikelihood(model::StatisticalModel, observation)\n\nReturn the contribution of observation to the log-likelihood of model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.nullloglikelihood","page":"API","title":"StatsBase.nullloglikelihood","text":"loglikelihood(model::StatisticalModel)\n\nReturn the log-likelihood of the null model corresponding to model. This is usually the model containing only the intercept.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.stderror","page":"API","title":"StatsBase.stderror","text":"stderror(model::StatisticalModel)\n\nReturn the standard errors for the coefficients of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.vcov","page":"API","title":"StatsBase.vcov","text":"vcov(model::StatisticalModel)\n\nReturn the variance-covariance matrix for the coefficients of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.weights","page":"API","title":"StatsBase.weights","text":"weights(vs)\n\nConstruct a Weights vector from array vs. See the documentation for Weights for more details.\n\n\n\n\n\nweights(model::StatisticalModel)\n\nReturn the weights used in the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.workingweights","page":"API","title":"RobustModels.workingweights","text":"workingweights(m::RobustLinearModel)\n\nThe robust weights computed by the model.\n\nThis can be used to detect outliers, as outliers weights are lower than the weights of valid data points.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.fitted","page":"API","title":"StatsBase.fitted","text":"fitted(model::RegressionModel)\n\nReturn the fitted values of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.predict","page":"API","title":"StatsBase.predict","text":"predict(model::RegressionModel, [newX])\n\nForm the predicted response of model. An object with new covariate values newX can be supplied, which should have the same type and structure as that used to fit model; e.g. for a GLM it would generally be a DataFrame with the same variable names as the original predictors.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.leverage","page":"API","title":"StatsBase.leverage","text":"leverage(model::RegressionModel)\n\nReturn the diagonal of the projection matrix of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.modelmatrix","page":"API","title":"StatsBase.modelmatrix","text":"modelmatrix(model::RegressionModel)\n\nReturn the model matrix (a.k.a. the design matrix).\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.projectionmatrix","page":"API","title":"RobustModels.projectionmatrix","text":"projectionmatrix(m::RobustLinearModel)\n\nThe robust projection matrix from the predictor: X (X' W X)⁻¹ X' W\n\n\n\n\n\n","category":"function"},{"location":"api/#GLM.dispersion-Tuple{RobustLinearModel, Bool}","page":"API","title":"GLM.dispersion","text":"dispersion(m::RobustLinearModel, sqr::Bool=false)\n\nThe dispersion is the (weighted) sum of robust residuals. If sqr is true, return the squared dispersion.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.response","page":"API","title":"StatsBase.response","text":"response(model::RegressionModel)\n\nReturn the model response (a.k.a. the dependent variable).\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.residuals","page":"API","title":"StatsBase.residuals","text":"residuals(model::RegressionModel)\n\nReturn the residuals of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.scale","page":"API","title":"RobustModels.scale","text":"scale(m::RobustLinearModel, sqr::Bool=false)\n\nThe robust scale estimate used for the robust estimation.\n\nIf sqr is true, the square of the scale is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.tauscale","page":"API","title":"RobustModels.tauscale","text":"tauscale(m::RobustLinearModel, sqr::Bool=false; kwargs...)\n\nThe robust τ-scale that is minimized in τ-estimation.\n\nIf sqr is true, the square of the τ-scale is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.location_variance","page":"API","title":"RobustModels.location_variance","text":"location_variance(r::RobustLinResp, sqr::Bool=false)\n\nCompute the part of the variance of the coefficients β that is due to the encertainty from the location.\n\nIf sqr is false, return the standard deviation instead.\n\nFrom Maronna et al., Robust Statistics: Theory and Methods, Equation 4.49\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.Estimator","page":"API","title":"RobustModels.Estimator","text":"Estimator(m::RobustLinearModel)\n\nThe robust estimator object used to fit the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#Estimators","page":"API","title":"Estimators","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"MEstimator\nRobustModels.L1Estimator\nL2Estimator\nSEstimator\nMMEstimator\nTauEstimator\nGeneralizedQuantileEstimator\nExpectileEstimator\nRobustModels.QuantileEstimator","category":"page"},{"location":"api/#RobustModels.MEstimator","page":"API","title":"RobustModels.MEstimator","text":"MEstimator{L<:LossFunction} <: AbstractEstimator\n\nM-estimator for a given loss function.\n\nThe M-estimator is obtained by minimizing the loss function:\n\nhatmathbfbeta = undersetmathbfbetatextrmargmin sum_i=1^n rholeft(dfracr_ihatsigmaright)\n\nwith the residuals  mathbfr = mathbfy - mathbfX mathbfbeta , and a robust scale estimate hatsigma.\n\nFields\n\nloss: the LossFunction used for the robust estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.L1Estimator","page":"API","title":"RobustModels.L1Estimator","text":"L1Estimator is a shorthand name for MEstimator{L1Loss}. Using exact QuantileRegression should be prefered.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.L2Estimator","page":"API","title":"RobustModels.L2Estimator","text":"L2Estimator is a shorthand name for MEstimator{L2Loss}, the non-robust OLS.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.SEstimator","page":"API","title":"RobustModels.SEstimator","text":"SEstimator{L<:BoundedLossFunction} <: AbstractEstimator\n\nS-estimator for a given bounded loss function.\n\nThe S-estimator is obtained by minimizing the scale estimate:\n\nhatmathbfbeta = undersetmathbfbetatextrmargmin  hatsigma^2\n\nwhere the robust scale estimate hatsigma is solution of:\n\ndfrac1n sum_i=1^n rholeft(dfracr_ihatsigmaright) = delta\n\nwith the residuals  mathbfr = mathbfy - mathbfX mathbfbeta , rho is a bounded loss function with  undersetr to inftylim rho(r) = 1 and delta is the finite breakdown point, usually 0.5.\n\nFields\n\nloss: the LossFunction used for the robust estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.MMEstimator","page":"API","title":"RobustModels.MMEstimator","text":"MMEstimator{L1<:BoundedLossFunction, L2<:LossFunction} <: AbstractEstimator\n\nMM-estimator for the given loss functions.\n\nThe MM-estimator is obtained using a two-step process:\n\ncompute a robust scale estimate with a high breakdown point using a S-estimate and the loss function L1.\ncompute an efficient estimate using a M-estimate with the loss function L2.\n\nFields\n\nloss1: the BoundedLossFunction used for the high breakdown point S-estimation.\nloss2: the LossFunction used for the efficient M-estimation.\nscaleest: boolean specifying the if the estimation is in the S-estimation step (true)\n\nor the M-estimation step (false).\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.TauEstimator","page":"API","title":"RobustModels.TauEstimator","text":"TauEstimator{L1<:BoundedLossFunction, L2<:BoundedLossFunction} <: AbstractEstimator\n\nτ-estimator for the given loss functions.\n\nThe τ-estimator corresponds to a M-estimation, where the loss function is a weighted sum of a high breakdown point loss and an efficient loss. The weight is recomputed at every step of the Iteratively Reweighted Least Square, so the estimate is both robust (high breakdown point) and efficient.\n\nFields\n\nloss1: the high breakdown point BoundedLossFunction.\nloss2: the high efficiency BoundedLossFunction.\nw: the weight in the sum of losses: w . loss1 + loss2.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.GeneralizedQuantileEstimator","page":"API","title":"RobustModels.GeneralizedQuantileEstimator","text":"GeneralizedQuantileEstimator{L<:LossFunction} <: AbstractQuantileEstimator\n\nGeneralized Quantile Estimator is an M-Estimator with asymmetric loss function.\n\nFor L1Loss, this corresponds to quantile regression (although it is better to use quantreg for quantile regression because it gives the exact solution).\n\nFor L2Loss, this corresponds to Expectile regression (see ExpectileEstimator).\n\nFields\n\nloss: the LossFunction.\nτ: the quantile value to estimate, between 0 and 1.\n\nProperties\n\ntau, q, quantile are aliases for τ.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.ExpectileEstimator","page":"API","title":"RobustModels.ExpectileEstimator","text":"The expectile estimator is a generalization of the L2 estimator, for other quantile τ ∈ [0,1].\n\n[1] Schnabel, Eilers - Computational Statistics and Data Analysis 53 (2009) 4168–4177 - Optimal expectile smoothing doi:10.1016/j.csda.2009.05.002\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.QuantileEstimator","page":"API","title":"RobustModels.QuantileEstimator","text":"Non-exact quantile estimator, GeneralizedQuantileEstimator{L1Loss}. Prefer using QuantileRegression\n\n\n\n\n\n","category":"type"},{"location":"api/#Loss-functions","page":"API","title":"Loss functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"L2Loss\nL1Loss\nHuberLoss\nL1L2Loss\nFairLoss\nLogcoshLoss\nArctanLoss\nCauchyLoss\nGemanLoss\nWelschLoss\nTukeyLoss\nYohaiZamarLoss","category":"page"},{"location":"api/#RobustModels.L2Loss","page":"API","title":"RobustModels.L2Loss","text":"The (convex) L2 loss function is that of the standard least squares problem.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.L1Loss","page":"API","title":"RobustModels.L1Loss","text":"The standard L1 loss function takes the absolute value of the residual, and is convex but non-smooth. It is not a real L1 loss but a Huber loss with very small tuning constant.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.HuberLoss","page":"API","title":"RobustModels.HuberLoss","text":"The convex Huber loss function switches from between quadratic and linear cost/loss function at a certain cutoff.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.L1L2Loss","page":"API","title":"RobustModels.L1L2Loss","text":"The convex L1-L2 loss interpolates smoothly between L2 behaviour for small residuals and L1 for outliers.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.FairLoss","page":"API","title":"RobustModels.FairLoss","text":"The (convex) \"fair\" loss switches from between quadratic and linear cost/loss function at a certain cutoff, and is C3 but non-analytic.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.LogcoshLoss","page":"API","title":"RobustModels.LogcoshLoss","text":"The convex Log-Cosh loss function log(cosh(r))\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.ArctanLoss","page":"API","title":"RobustModels.ArctanLoss","text":"The convex Arctan loss function r * arctan(r) - 1/2*log(1 + r^2)\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.CauchyLoss","page":"API","title":"RobustModels.CauchyLoss","text":"The non-convex Cauchy loss function switches from between quadratic behaviour to logarithmic tails. This rejects outliers but may result in multiple minima. For scale estimate, r.ψ(r) is used as a loss, which is the same as for Geman loss.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.GemanLoss","page":"API","title":"RobustModels.GemanLoss","text":"The non-convex Geman-McClure for strong supression of outliers and does not guarantee a unique solution. For the S-Estimator, it is equivalent to the Cauchy loss.\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.WelschLoss","page":"API","title":"RobustModels.WelschLoss","text":"The non-convex Welsch for strong supression of ourliers and does not guarantee a unique solution\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.TukeyLoss","page":"API","title":"RobustModels.TukeyLoss","text":"The non-convex Tukey biweight estimator which completely suppresses the outliers, and does not guaranty a unique solution\n\n\n\n\n\n","category":"type"},{"location":"api/#RobustModels.YohaiZamarLoss","page":"API","title":"RobustModels.YohaiZamarLoss","text":"The non-convex (and bounded) optimal Yohai-Zamar loss function that minimizes the estimator bias. It was originally introduced in  Optimal locally robust M-estimates of regression (1997) by Yohai and Zamar with a slightly different formula.\n\n\n\n\n\n","category":"type"},{"location":"api/#Estimator-and-Loss-functions-methods","page":"API","title":"Estimator and Loss functions methods","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"RobustModels.rho\nRobustModels.psi\nRobustModels.psider\nRobustModels.weight\nRobustModels.values\nRobustModels.estimator_norm\nRobustModels.estimator_bound\ntuning_constant\nRobustModels.isconvex\nRobustModels.isbounded\nRobustModels.estimator_high_breakdown_point_constant\nRobustModels.estimator_high_efficiency_constant\nRobustModels.efficient_loss\nRobustModels.robust_loss\nRobustModels.efficiency_tuning_constant\nRobustModels.mscale_loss\nRobustModels.breakdown_point_tuning_constant\nRobustModels.scale_estimate\nRobustModels.tau_efficiency_tuning_constant\nRobustModels.estimator_tau_efficient_constant\nloss\nRobustModels.set_SEstimator\nRobustModels.set_MEstimator\nRobustModels.update_weight!\nRobustModels.tau_scale_estimate\nRobustModels.quantile_weight","category":"page"},{"location":"api/#RobustModels.rho","page":"API","title":"RobustModels.rho","text":"The loss function ρ for the M-estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.psi","page":"API","title":"RobustModels.psi","text":"The influence function ψ is the derivative of the loss function for the M-estimator, multiplied by the square of the tuning constant.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.psider","page":"API","title":"RobustModels.psider","text":"The derivative of ψ, used for asymptotic estimates.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.weight","page":"API","title":"RobustModels.weight","text":"The weights for IRLS, the function ψ divided by r.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.values","page":"API","title":"RobustModels.values","text":"Faster version if you need ρ, ψ and w in the same call\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.estimator_norm","page":"API","title":"RobustModels.estimator_norm","text":"The integral of exp(-ρ) used for calculating the full-loglikelihood\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.estimator_bound","page":"API","title":"RobustModels.estimator_bound","text":"The limit at ∞ of the loss function. Used for scale estimation of bounded loss.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.tuning_constant","page":"API","title":"RobustModels.tuning_constant","text":"The tuning constant of the loss function, can be optimized to get efficient or robust estimates.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.isconvex","page":"API","title":"RobustModels.isconvex","text":"Boolean if the estimator or loss function is convex\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.isbounded","page":"API","title":"RobustModels.isbounded","text":"Boolean if the estimator or loss function is bounded\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.estimator_high_breakdown_point_constant","page":"API","title":"RobustModels.estimator_high_breakdown_point_constant","text":"The tuning constant associated to the loss that gives an efficient M-estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.estimator_high_efficiency_constant","page":"API","title":"RobustModels.estimator_high_efficiency_constant","text":"The tuning constant associated to the loss that gives a robust (high breakdown point) M-estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.efficient_loss","page":"API","title":"RobustModels.efficient_loss","text":"The loss initialized with an efficient tuning constant\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.robust_loss","page":"API","title":"RobustModels.robust_loss","text":"The loss initialized with a robust (high breakdown point) tuning constant\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.efficiency_tuning_constant","page":"API","title":"RobustModels.efficiency_tuning_constant","text":"The tuning constant c is computed so the efficiency for Normal distributed residuals is 0.95. The efficiency of the mean estimate μ is defined by:\n\neff_μ = (E[ψ'])²/E[ψ²]\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.mscale_loss","page":"API","title":"RobustModels.mscale_loss","text":"mscale_loss(loss::L, x)\n\nThe rho-function that is used for M-scale estimation.\n\nFor monotone (convex) functions, χ(r) = r.ψ(r).\n\nFor bounded functions, χ(r) = ρ(r)/ρ(∞) so χ(∞) = 1.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.breakdown_point_tuning_constant","page":"API","title":"RobustModels.breakdown_point_tuning_constant","text":"The M-estimate of scale is computed by solving:\n\ndfrac1n sum_i chileft( dfracr_ihatsigmaright) = delta\n\nFor monotone (convex) functions, χ(r) = r.ψ(r) and δ is defined as E[χ(r)] = δ for the Normal distribution N(0,1) For bounded functions, χ(r) = ρ(r)/ρ(∞) with χ(∞) = 1 and δ = E[χ]/χ(∞) with expectation w.r.t. Normal density.\n\nThe tuning constant c corresponding to a high breakdown point (0.5) is such that δ = 1/2, from  1/n Σ χ(r/ŝ) = δ\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.scale_estimate","page":"API","title":"RobustModels.scale_estimate","text":"scale_estimate(loss, res; σ0=1.0, wts=[], verbose=false,\n                         order=1, approx=false, nmax=30, \n                         rtol=1e-4, atol=0.1)\n\nCompute the M-scale estimate from the loss function. If the loss is bounded, ρ is used as the function χ in the sum, otherwise r.ψ(r) is used if the loss is not bounded, to coincide with the Maximum Likelihood Estimator. Also, for bounded estimator, because f(s) = 1/(nδ) Σ ρ(ri/s) is decreasing the iteration step is not using the weights but is multiplicative.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.tau_efficiency_tuning_constant","page":"API","title":"RobustModels.tau_efficiency_tuning_constant","text":"Compute the tuning constant that corresponds to a high breakdown point for the τ-estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.estimator_tau_efficient_constant","page":"API","title":"RobustModels.estimator_tau_efficient_constant","text":"The tuning constant associated to the loss that gives a robust τ-estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.loss","page":"API","title":"RobustModels.loss","text":"The loss function used for the estimation\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.set_SEstimator","page":"API","title":"RobustModels.set_SEstimator","text":"MEstimator, set to S-Estimation phase\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.set_MEstimator","page":"API","title":"RobustModels.set_MEstimator","text":"MEstimator, set to M-Estimation phase\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.update_weight!","page":"API","title":"RobustModels.update_weight!","text":"update_weight!(E::TauEstimator, res::AbstractArray{T}; wts::AbstractArray{T}=T[])\n\nUpdate the weight between the two estimators of a τ-estimator using the scaled residual.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.tau_scale_estimate","page":"API","title":"RobustModels.tau_scale_estimate","text":"tau_scale_estimate!(E::TauEstimator, res::AbstractArray{T}, σ::Real, sqr::Bool=false;\n                    wts::AbstractArray{T}=T[], bound::AbstractFloat=0.5) where {T<:AbstractFloat}\n\nThe τ-scale estimate, where σ is the scale estimate from the robust M-scale. If sqr is true, return the squared value.\n\n\n\n\n\n","category":"function"},{"location":"api/#RobustModels.quantile_weight","page":"API","title":"RobustModels.quantile_weight","text":"quantile_weight(τ::Real, r::Real)\n\nWrapper function to compute quantile-like loss function.\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"DocTestSetup = quote\n    using DataFrames, GLM\n    using RobustModels\nend","category":"page"},{"location":"examples/#Robust-linear-regression","page":"Examples","title":"Robust linear regression","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"julia> using DataFrames, RobustModels\n\njulia> data = DataFrame(X=[1,2,3,4,5,6], Y=[2,4,7,8,9,13])\n6×2 DataFrame\n Row │ X      Y\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      2\n   2 │     2      4\n   3 │     3      7\n   4 │     4      8\n   5 │     5      9\n   6 │     6     13\n\njulia> ols = rlm(@formula(Y ~ X), data, MEstimator{L2Loss}())\nRobust regression with M-Estimator(L2Loss())\n\nY ~ 1 + X\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  0.0666667    0.781533   0.09    0.9361   -2.10322    2.23655\nX            2.02857      0.200679  10.11    0.0005    1.4714     2.58575\n─────────────────────────────────────────────────────────────────────────\n\njulia> round.(stderror(ols), digits=5)\n2-element Array{Float64,1}:\n 0.78153\n 0.20068\n\njulia> round.(predict(ols), digits=5)\n6-element Array{Float64,1}:\n  2.09524\n  4.12381\n  6.15238\n  8.18095\n 10.20952\n 12.2381\n\njulia> data[5, :Y] = 1; data\n6×2 DataFrame\n Row │ X      Y\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      2\n   2 │     2      4\n   3 │     3      7\n   4 │     4      8\n   5 │     5      1\n   6 │     6     13\n \njulia> rob = rlm(@formula(Y ~ X), data, MMEstimator{TukeyLoss}(); σ0=:mad)\nRobust regression with MM-Estimator(TukeyLoss(1.5476), TukeyLoss(4.685))\n\nY ~ 1 + X\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  -0.184561    0.514249  -0.36    0.7378   -1.61235    1.24322\nX             2.18005     0.14112   15.45    0.0001    1.78824    2.57186\n─────────────────────────────────────────────────────────────────────────\n","category":"page"},{"location":"manual/#Manual","page":"Manual","title":"Manual","text":"","category":"section"},{"location":"manual/#Installation","page":"Manual","title":"Installation","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Pkg.add(\"RobustModels\")","category":"page"},{"location":"manual/#Fitting-robust-models","page":"Manual","title":"Fitting robust models","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The API is consistent with GLM package. To fit a robust model, use the function, rlm(formula, data, estimator; initial_scale=:mad), where,","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"formula: uses column symbols from the DataFrame data, for example, if propertynames(data)=[:Y,:X1,:X2],then a valid formula is @formula(Y ~ X1 + X2). An intercept is included by default.\ndata: a DataFrame which may contain missing values\nestimator: chosen from\nMEstimator\nSEstimator\nMMEstimator\nTauEstimator\nGeneralizedQuantileEstimator","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Supported loss functions are:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"L2Loss: ρ(r) = ½ r², like ordinary OLS.\nL1Loss: ρ(r) = |r|, non-differentiable estimator also know as Least absolute deviations. Prefer the QuantileRegression solver.\nHuberLoss: ρ(r) = if (r<c); ½(r/c)² else |r|/c - ½ end, convex estimator that behaves as L2 cost for small residuals and L1 for large esiduals and outliers.\nL1L2Loss: ρ(r) = √(1 + (r/c)²) - 1, smooth version of HuberLoss.\nFairLoss: ρ(r) = |r|/c - log(1 + |r|/c), smooth version of HuberLoss.\nLogcoshLoss: ρ(r) = log(cosh(r/c)), smooth version of HuberLoss.\nArctanLoss: ρ(r) = r/c * atan(r/c) - ½ log(1+(r/c)²), smooth version of HuberLoss.\nCauchyLoss: ρ(r) = log(1+(r/c)²), non-convex estimator, that also corresponds to a Student's-t distribution (with fixed degree of freedom). It suppresses outliers more strongly but it is not sure to converge.\nGemanLoss: ρ(r) = ½ (r/c)²/(1 + (r/c)²), non-convex and bounded estimator, it suppresses outliers more strongly.\nWelschLoss: ρ(r) = ½ (1 - exp(-(r/c)²)), non-convex and bounded estimator, it suppresses outliers more strongly.\nTukeyLoss: ρ(r) = if r<c; ⅙(1 - (1-(r/c)²)³) else ⅙ end, non-convex and bounded estimator, it suppresses outliers more strongly and it is the prefered estimator for most cases.\nYohaiZamarLoss: ρ(r) is quadratic for r/c < 2/3 and is bounded to 1; non-convex estimator, it is optimized to have the lowest bias for a given efficiency.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"An estimator is constructed from an estimator type and a loss, e.g. MEstimator{TukeyLoss}().","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"For GeneralizedQuantileEstimator, the quantile should be specified with τ (0.5 by default), e.g. GeneralizedQuantileEstimator{HuberLoss}(0.2).","category":"page"},{"location":"manual/#Methods-applied-to-fitted-models","page":"Manual","title":"Methods applied to fitted models","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Many of the methods are consistent with GLM.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"nobs: number of observations\ndof_residual: degrees of freedom for residuals\ndof: degrees of freedom of the model, defined by nobs(m) - dof_residual(m)\ncoef: estimate of the coefficients in the model\npredict : obtain predicted values of the dependent variable from the fitted model\ndeviance/nulldeviance: measure of the model (null model, respectively) fit\nstderror: standard errors of the coefficients\nconfint: confidence intervals for the fitted coefficients\nscale: the scale estimate from the model\nworkingweights: the weights for each observation from the robust estimate. Outliers have low weights\nleverage: the vector of leverage score for each observation\nvcov: estimated variance-covariance matrix of the coefficient estimates","category":"page"},{"location":"manual/#Separation-of-response-object-and-predictor-object","page":"Manual","title":"Separation of response object and predictor object","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Building upon GLM separation of the response and predictor objects, this package implements a new RobustLinResp object to compute the residuals. There are currently two available predictor objects: DensePredChol/SparsePredChol (imported from GLM) and DensePredCG/SparsePredCG that use the iterative Conjugate Gradient methods, cg! and lsqr! from the IterativeSolvers package that is faster and more accurate than Cholesky method for very large matrices. The predictor that is used depends on the model matrix type and the method argument of the fit/fit!/rlm methods.","category":"page"},{"location":"#RobustModels-Documentation","page":"Home","title":"RobustModels Documentation","text":"","category":"section"},{"location":"#Package-summary","page":"Home","title":"Package summary","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Robust linear models in Julia","category":"page"}]
}
